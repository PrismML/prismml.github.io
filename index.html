<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>PrismML | Ultra‑Efficient Foundation Models</title>
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <style>
    body{font-family:system-ui,Helvetica,Arial,sans-serif;margin:0;padding:0;color:#111;background:#fff;}
    header{padding:4rem 2rem;text-align:center;background:#f5f5f8;}
    h1{margin:0;font-size:3rem;font-weight:700;}
    h2{font-weight:400;color:#555;}
    section{max-width:840px;margin:4rem auto;padding:0 1rem;line-height:1.6;}
    code{background:#eef;border-radius:3px;padding:0.1em 0.3em;}
    footer{padding:2rem 0;text-align:center;font-size:0.85rem;color:#777;border-top:1px solid #eee;}
  </style>
</head>
<body>
  <header>
    <h1>PrismML</h1>
    <h2>50 × structured & unstructured pruning | 1‑bit quantization | trained from scratch</h2>
  </header>

  <section>
    <p><strong>Mission.</strong> We build lean foundation models that preserve accuracy while slashing FLOPs and memory.</p>

    <h3>Our recipe</h3>
    <ul>
      <li><b>Structured pruning.</b> Block‑sparse kernels for predictable speed‑ups on GPU/TPU.</li>
      <li><b>Unstructured pruning.</b> Magnitude‑ and saliency‑based masks to reach 50 × parameter reduction.</li>
      <li><b>1‑bit quantization.</b> XNOR‑GEMM friendly weights with learned per‑channel scaling.</li>
      <li><b>Scratch training.</b> No catastrophic forgetting—models converge directly in the compressed regime.</li>
    </ul>

    <h3>Try it</h3>
    <pre><code>pip install prismml</code></pre>
  </section>

  <footer>© 2025 PrismML · All rights reserved</footer>
</body>
</html> 